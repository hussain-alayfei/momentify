{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11334862,"sourceType":"datasetVersion","datasetId":7090382}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 🎬 Momentify: AI-Powered Sports Highlight Generator\n\n**Momentify** is an intelligent video processing tool that automatically identifies and extracts key moments from sports videos. By combining speech transcription, natural language understanding, and video editing, Momentify allows users to upload a match video and request specific highlights—such as \"Messi goals\" or \"penalty kicks\"—which are then compiled into a final highlight reel.\n\n---\n\n### 📚 Notebook Structure\n\n1. **Install Dependencies**  \n   Required Python libraries such as Whisper, MoviePy, Gradio, and Groq SDK.\n\n2. **Import Libraries & Load API Keys**  \n   Load necessary modules and securely retrieve your Groq API key from Kaggle Secrets.\n\n3. **Initialize Models**  \n   Load the Whisper speech-to-text model and Groq LLM client.\n\n4. **Define Core Functions**  \n   - `transcribe_video()` – Transcribes audio from uploaded video  \n   - `extract_highlights()` – Uses Groq (LLaMA 4) to find highlight-worthy timestamps  \n   - `process_video()` – Cuts the video clips and combines them\n\n5. **Build Gradio Interface**  \n   Interactive web UI for uploading video, entering a query, and previewing the result.\n\n6. **Launch App**  \n   Runs the full application through a simple user-friendly interface.\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# 📦 Install Dependencies\n\nThese are the required packages:\n- `gradio` for UI\n- `moviepy` for video editing\n- `whisper` for transcription\n- `groq` for using LLaMA 3/4 models via Groq API\n","metadata":{}},{"cell_type":"code","source":"!pip install -q gradio\n!pip install -q moviepy\n!pip install -q git+https://github.com/openai/whisper.git\n!pip install -q groq","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-04-09T16:35:28.637491Z","iopub.execute_input":"2025-04-09T16:35:28.637866Z","iopub.status.idle":"2025-04-09T16:35:49.929006Z","shell.execute_reply.started":"2025-04-09T16:35:28.637840Z","shell.execute_reply":"2025-04-09T16:35:49.927669Z"},"trusted":true},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":114},{"cell_type":"markdown","source":"# 📚 Import Libraries\n\nWe import everything needed:\n- Whisper for transcription\n- MoviePy for video editing\n- Groq for highlight extraction\n- Gradio for the interactive interface\n","metadata":{}},{"cell_type":"code","source":"import whisper\nimport gradio as gr\nimport moviepy.editor as mp\nimport json\nimport re\nimport os\nfrom groq import Groq\nfrom kaggle_secrets import UserSecretsClient","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T16:37:21.178431Z","iopub.execute_input":"2025-04-09T16:37:21.178802Z","iopub.status.idle":"2025-04-09T16:37:21.183089Z","shell.execute_reply.started":"2025-04-09T16:37:21.178772Z","shell.execute_reply":"2025-04-09T16:37:21.182135Z"}},"outputs":[],"execution_count":116},{"cell_type":"markdown","source":"# 🔐 Load GROQ API Key from Kaggle Secrets\n\nThis is safer than putting your key directly in the code.\nYou must add `GROQ_API_KEY` to your env Secrets.\n","metadata":{}},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\ngroq_api_key = user_secrets.get_secret(\"GROQ_API_KEY\")\nclient = Groq(api_key=groq_api_key)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T16:38:01.349599Z","iopub.execute_input":"2025-04-09T16:38:01.349952Z","iopub.status.idle":"2025-04-09T16:38:01.572577Z","shell.execute_reply.started":"2025-04-09T16:38:01.349925Z","shell.execute_reply":"2025-04-09T16:38:01.570330Z"}},"outputs":[],"execution_count":117},{"cell_type":"markdown","source":"# 🗣️ Load Whisper Model (once)\n\nWe use the medium-sized model for a good balance between speed and accuracy.","metadata":{}},{"cell_type":"code","source":"model = whisper.load_model(\"medium\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T16:38:22.547602Z","iopub.execute_input":"2025-04-09T16:38:22.547979Z","iopub.status.idle":"2025-04-09T16:38:35.832222Z","shell.execute_reply.started":"2025-04-09T16:38:22.547948Z","shell.execute_reply":"2025-04-09T16:38:35.831505Z"}},"outputs":[],"execution_count":118},{"cell_type":"markdown","source":"# 🎧 Transcribe Uploaded Video with Whisper\n\nThis function takes an MP4 file and returns the transcript text and timestamps.\n","metadata":{}},{"cell_type":"code","source":"def transcribe_video(file_path):\n    result = model.transcribe(file_path, verbose=True)\n    return result['text'], result['segments']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T16:38:41.837431Z","iopub.execute_input":"2025-04-09T16:38:41.837784Z","iopub.status.idle":"2025-04-09T16:38:41.842043Z","shell.execute_reply.started":"2025-04-09T16:38:41.837745Z","shell.execute_reply":"2025-04-09T16:38:41.841078Z"}},"outputs":[],"execution_count":119},{"cell_type":"markdown","source":"# 🧠 Extract Highlights from Transcript using LLaMA 4 (Groq)\n\nThis sends the transcript and user query to Groq API to get segment timestamps.\nIt expects structured JSON and handles streamed responses.\n","metadata":{}},{"cell_type":"code","source":"def extract_highlights(transcript_segments, user_query):\n    transcript_text = \"\\n\".join([\n        f\"[{int(seg['start'])} --> {int(seg['end'])}] {seg['text']}\"\n        for seg in transcript_segments\n    ])\n\n    prompt = f\"\"\"\nYou are a JSON-only assistant.\n\nReturn only valid JSON in the following format:\n\n{{\n  \"segments\": [\n    {{ \"start\": 52, \"end\": 58 }},\n    {{ \"start\": 108, \"end\": 110 }}\n  ]\n}}\n\nNo markdown, no explanation, no wrapping. Just plain JSON.\nUser query: {user_query}\nTranscript:\n{transcript_text}\n\"\"\"\n\n    completion = client.chat.completions.create(\n        model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n        messages=[{\"role\": \"user\", \"content\": prompt}],\n        temperature=1,\n        max_tokens=1024,\n        top_p=1,\n        stream=True,\n        stop=None,\n    )\n\n    streamed_response = \"\"\n    for chunk in completion:\n        streamed_response += chunk.choices[0].delta.content or \"\"\n\n    raw_json = streamed_response.strip()\n\n    try:\n        return json.loads(raw_json)\n    except json.JSONDecodeError:\n        raise ValueError(\"❌ Groq did not return valid JSON:\\n\" + raw_json)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T17:13:49.192972Z","iopub.execute_input":"2025-04-09T17:13:49.193476Z","iopub.status.idle":"2025-04-09T17:13:49.218066Z","shell.execute_reply.started":"2025-04-09T17:13:49.193437Z","shell.execute_reply":"2025-04-09T17:13:49.213653Z"}},"outputs":[],"execution_count":129},{"cell_type":"markdown","source":"# 🎬 Process Video: Transcribe → Detect Highlights → Cut Clips\n\nThis function:\n1. Transcribes the video\n2. Extracts highlights with Groq\n3. Cuts clips using MoviePy\n4. Returns steps and the final video path\n","metadata":{}},{"cell_type":"code","source":"def process_video(video_file, user_query):\n    steps = []\n    steps.append(\"🎬 **Step 1:** Video uploaded.\")\n\n    video_path = video_file\n    steps.append(\"🔍 **Step 2:** Transcribing video with Whisper...\")\n    transcript_text, transcript_segments = transcribe_video(video_path)\n\n    steps.append(\"🧠 **Step 3:** Extracting highlights using LLaMA 4...\")\n    results = extract_highlights(transcript_segments, user_query)\n\n    segments = [(max(0, seg[\"start\"] - 5), seg[\"end\"] + 5) for seg in results[\"segments\"]]\n    steps.append(f\"✂️ **Step 4:** Extracting {len(segments)} clips from video...\")\n\n    video = mp.VideoFileClip(video_path)\n    clips = [video.subclip(start, end) for start, end in segments]\n    final_clip = mp.concatenate_videoclips(clips)\n\n    output_path = \"highlight_output.mp4\"\n    steps.append(\"🎞️ **Step 5:** Rendering final highlight video...\")\n    final_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", verbose=False, logger=None)\n\n    steps.append(\"✅ **Step 6:** Done! Here is your final highlight video.\")\n    return \"\\n\\n\".join(steps), output_path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T17:14:10.497387Z","iopub.execute_input":"2025-04-09T17:14:10.497672Z","iopub.status.idle":"2025-04-09T17:14:10.504140Z","shell.execute_reply.started":"2025-04-09T17:14:10.497649Z","shell.execute_reply":"2025-04-09T17:14:10.503124Z"}},"outputs":[],"execution_count":133},{"cell_type":"markdown","source":"# 🌐 Gradio UI Interface\n\nThis creates a web app where users can upload a video and enter a query.\nThe processed highlight video is displayed after processing.\n","metadata":{}},{"cell_type":"code","source":"with gr.Blocks(theme=gr.themes.Soft()) as demo:\n    gr.Markdown(\"# ⚽️ Momentify\")\n    gr.Markdown(\"Upload a sports video and ask for highlights (e.g., `Messi goals`, `penalties`, etc.)\")\n\n    with gr.Row():\n        with gr.Column():\n            video_input = gr.Video(label=\"📁 Upload MP4 Video\")\n            query_input = gr.Textbox(label=\"📝 Highlight Query\", placeholder=\"e.g. Messi goals\")\n            submit_btn = gr.Button(\"🚀 Generate Highlights\")\n            output_status = gr.Markdown()\n\n        with gr.Column():\n            output_video = gr.Video(label=\"🎯 Final Highlight Video\")\n\n    submit_btn.click(\n        fn=process_video,\n        inputs=[video_input, query_input],\n        outputs=[output_status, output_video]\n    )\n\ndemo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-09T18:00:27.858338Z","iopub.execute_input":"2025-04-09T18:00:27.859083Z","iopub.status.idle":"2025-04-09T18:00:30.368551Z","shell.execute_reply.started":"2025-04-09T18:00:27.859049Z","shell.execute_reply":"2025-04-09T18:00:30.367897Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7884\nKaggle notebooks require sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://6f364b792cb47d9619.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://6f364b792cb47d9619.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":137,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":137},{"cell_type":"markdown","source":"### *Programmed by Hussain Alyafei*\n### *Thank you!*","metadata":{}}]}